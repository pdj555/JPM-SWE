# High-Performance Transaction Processing Platform - Spark Analytics Configuration
#
# Real-time streaming analytics configuration for enterprise transaction processing.
# Optimized for 100K+ events/second with sub-30 second end-to-end processing latency.

# Application Configuration
spark {
  app {
    name = "transaction-platform-analytics"
    master = "local[*]"
    driver.memory = "2g"
    executor.memory = "4g"
    executor.cores = 4
    executor.instances = 8
  }
  
  # Checkpointing Configuration
  checkpoint {
    location = "/tmp/transaction-platform-checkpoint"
    interval = "30 seconds"
  }
  
  # Streaming Configuration
  streaming {
    batchDuration = "5 seconds"
    stopGracefullyOnShutdown = true
    backpressure.enabled = true
    kafka.maxRatePerPartition = 10000
    
    # State store configuration for reliability
    stateStore {
      maintenanceInterval = "60 seconds"
      minDeltasForSnapshot = 10
    }
  }
  
  # Serialization Configuration
  serializer = "org.apache.spark.serializer.KryoSerializer"
  kryo.registrationRequired = false
  
  # Adaptive Query Execution
  sql.adaptive {
    enabled = true
    coalescePartitions.enabled = true
    skewJoin.enabled = true
    localShuffleReader.enabled = true
  }
  
  # Memory Management
  sql.execution.arrow.pyspark.enabled = true
  sql.execution.arrow.maxRecordsPerBatch = 10000
  
  # Dynamic Allocation
  dynamicAllocation {
    enabled = true
    minExecutors = 2
    maxExecutors = 20
    initialExecutors = 4
  }
}

# Kafka Configuration
kafka {
  bootstrap.servers = "localhost:9092"
  topic = "transactions.platform"
  
  # Consumer Configuration
  consumer {
    group.id = "transaction-platform-analytics"
    auto.offset.reset = "latest"
    enable.auto.commit = false
    session.timeout.ms = 30000
    request.timeout.ms = 40000
    max.poll.records = 50000
    max.poll.interval.ms = 300000
    
    # Reliability Settings
    fetch.min.bytes = 1048576
    fetch.max.wait.ms = 500
    heartbeat.interval.ms = 3000
  }
  
  # Producer Configuration (for output streams)
  producer {
    acks = "all"
    retries = 2147483647
    batch.size = 32768
    linger.ms = 5
    buffer.memory = 67108864
    compression.type = "lz4"
    enable.idempotence = true
    max.in.flight.requests.per.connection = 5
  }
}

# Streaming Windows Configuration
streaming {
  # Watermark threshold for late data handling
  watermark {
    threshold = "5 minutes"
    allowedLateness = "10 minutes"
  }
  
  # Trigger intervals for different aggregations
  trigger {
    interval = "30 seconds"
    processingTime = "1 minute"
  }
  
  # Windowing configuration
  windows {
    tumbling {
      size = "1 minute"
      slide = "30 seconds"
    }
    session {
      gap = "5 minutes"
    }
  }
}

# Output Configuration
output {
  # Console sink for monitoring
  console {
    enabled = true
    truncate = false
    numRows = 20
  }
  
  # File sink for analytics
  file {
    enabled = true
    path = "/tmp/transaction-platform-output"
    format = "parquet"
    mode = "append"
    partitionBy = ["date", "hour"]
    
    # Compression settings
    compression = "snappy"
    maxRecordsPerFile = 1000000
  }
  
  # Database sink configuration
  database {
    enabled = false
    url = "jdbc:postgresql://localhost:5432/transaction_platform"
    driver = "org.postgresql.Driver"
    table = "aggregated_transactions"
    batchSize = 10000
  }
}

# Monitoring & Metrics Configuration
monitoring {
  metrics {
    enabled = true
    namespace = "transaction-platform.analytics"
    reporters = ["jmx", "graphite"]
    
    # Custom metrics configuration
    custom {
      transactionVolume = true
      processingLatency = true
      errorRates = true
      throughputMetrics = true
    }
  }
  
  # Health checks
  healthCheck {
    enabled = true
    interval = "30 seconds"
    timeout = "10 seconds"
  }
  
  # Logging configuration
  logging {
    level = "INFO"
    pattern = "%d{yyyy-MM-dd HH:mm:ss} %-5level %logger{36} - %msg%n"
    
    # Specific logger levels
    loggers = {
      "com.chase.transactionplatform" = "INFO"
      "org.apache.spark" = "WARN"
      "org.apache.kafka" = "WARN"
      "org.apache.hadoop" = "WARN"
    }
  }
} 